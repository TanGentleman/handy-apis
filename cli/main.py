#!/usr/bin/env python3
"""CLI tool to fetch documentation from the content-scraper API."""

import json
import os
import sys
import time
from typing import Annotated, Optional
from urllib.parse import urlparse

import httpx
import typer

# Import API URL and auth from config (generated by setup.py)
from config.settings import get_api_url, get_auth_headers

# Resolve API URL at module load - will fail with clear error if not configured
API_BASE = get_api_url()

# Main app and subcommands
app = typer.Typer(
    help="CLI tool to fetch and cache documentation as markdown.",
    no_args_is_help=True,
)
cache_app = typer.Typer(help="Cache management commands.")
app.add_typer(cache_app, name="cache")


@app.command()
def sites():
    """List all available site IDs."""
    resp = httpx.get(f"{API_BASE}/sites", timeout=30.0)
    resp.raise_for_status()
    data = resp.json()
    for site in data["sites"]:
        print(site["id"])


@app.command()
def discover(
    url: Annotated[str, typer.Argument(help="Full URL of a documentation page to analyze")],
):
    """Analyze a documentation page and suggest configuration.

    Prints comprehensive discovery results including framework detection,
    working copy buttons, ranked content selectors, link patterns,
    and a ready-to-use configuration snippet.
    """
    # Validate URL format
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        print(f"Error: Invalid URL format: {url}", file=sys.stderr)
        print("URL must include protocol (e.g., https://example.com)", file=sys.stderr)
        raise typer.Exit(1)

    if parsed.scheme not in ("http", "https"):
        print(f"Error: Unsupported protocol: {parsed.scheme}", file=sys.stderr)
        print("Only http:// and https:// URLs are supported", file=sys.stderr)
        raise typer.Exit(1)

    print(f"Analyzing {url}...\n", file=sys.stderr)

    try:
        resp = httpx.get(
            f"{API_BASE}/discover",
            params={"url": url},
            headers=get_auth_headers(),
            timeout=30.0,
        )
        resp.raise_for_status()
        data = resp.json()
    except httpx.HTTPStatusError as e:
        print(f"Error: HTTP {e.response.status_code}", file=sys.stderr)
        try:
            error_detail = e.response.json().get("detail", str(e))
            print(f"Details: {error_detail}", file=sys.stderr)
        except Exception:
            print(f"Details: {e}", file=sys.stderr)
        raise typer.Exit(1)
    except httpx.TimeoutException:
        print("Error: Request timed out (60s)", file=sys.stderr)
        print("The page may be slow to load or unresponsive", file=sys.stderr)
        raise typer.Exit(1)
    except httpx.HTTPError as e:
        print(f"Error: Failed to connect to API: {e}", file=sys.stderr)
        raise typer.Exit(1)

    # Parse URL for fallback suggestions
    parsed = urlparse(url)

    print("=" * 70)
    print(f"DISCOVERY RESULTS FOR: {url}")
    print("=" * 70)

    # Framework detection
    framework = data.get("framework", "unknown")
    print(f"\nFramework Detected: {framework.upper()}")
    print(f"Suggested Base URL: {data['base_url_suggestion']}")

    # Copy buttons section
    print("\n" + "-" * 70)
    print("COPY BUTTONS (tested with live page load):")
    print("-" * 70)
    copy_buttons = data.get("copy_buttons", [])
    if copy_buttons:
        working_buttons = [b for b in copy_buttons if b.get("works")]
        if working_buttons:
            print(f"\n  Found {len(working_buttons)} working copy button(s):\n")
            for i, btn in enumerate(working_buttons, 1):
                print(f"  {i}. {btn['selector']}")
                print(f"     Tested: {btn['chars']:,} chars extracted\n")

        failed_buttons = [b for b in copy_buttons if not b.get("works")]
        if failed_buttons:
            print(f"  Found {len(failed_buttons)} non-working button(s):")
            for btn in failed_buttons:
                error = btn.get("error", "unknown")[:60]
                print(f"    - {btn['selector']} (Error: {error})")
    else:
        print("  No copy buttons detected")

    # Content selectors section
    print("\n" + "-" * 70)
    print("CONTENT SELECTORS (ranked by quality):")
    print("-" * 70)
    content_selectors = data.get("content_selectors", [])
    if content_selectors:
        print(f"\n  Found {len(content_selectors)} viable selector(s):\n")
        for i, sel in enumerate(content_selectors[:5], 1):
            marker = "[RECOMMENDED]" if sel.get("recommended") else ""
            print(f"  {i}. {sel['selector']} {marker}")
            print(f"     {sel['text_chars']:,} text chars | {sel['chars']:,} HTML chars\n")
    else:
        print("  WARNING: No content selectors found with substantial content")

    # Link analysis section
    link_data = data.get("link_analysis", {})
    total_links = link_data.get("total_internal_links", 0)

    print("-" * 70)
    print("LINK ANALYSIS:")
    print("-" * 70)
    print(f"\n  Total internal links: {total_links}")

    patterns = link_data.get("path_patterns", [])
    if patterns:
        print("\n  Path patterns (by frequency):")
        for pattern, count in patterns[:5]:
            pct = (count / total_links * 100) if total_links > 0 else 0
            print(f"    {pattern:20} {count:4} links ({pct:.1f}%)")
    else:
        print("\n  No clear path patterns detected")

    samples = link_data.get("sample_links", [])
    if samples:
        print(f"\n  Sample links ({min(5, len(samples))} of {len(samples)}):")
        for link in samples[:5]:
            print(f"    {link}")

    # Generate suggested config
    print("\n" + "=" * 70)
    print("SUGGESTED CONFIGURATION:")
    print("=" * 70)

    # Determine best content extraction method
    if copy_buttons and any(b.get("works") for b in copy_buttons):
        best_copy = next(b for b in copy_buttons if b.get("works"))
        content_config = f'''  "content": {{
    "mode": "browser",
    "waitFor": "{best_copy['selector']}",
    "selector": "{best_copy['selector']}",
    "method": "click_copy"
  }}'''
        method_note = "  # Using copy button (most reliable)"
    elif content_selectors:
        best_selector = content_selectors[0]
        content_config = f'''  "content": {{
    "mode": "browser",
    "waitFor": "{best_selector['selector']}",
    "selector": "{best_selector['selector']}",
    "method": "inner_html"
  }}'''
        method_note = "  # Using content selector"
    else:
        content_config = '''  "content": {
    "mode": "browser",
    "selector": "main",
    "method": "inner_html"
  }'''
        method_note = "  # WARNING: Using fallback selector - may need adjustment"

    # Determine link crawling config
    if patterns:
        best_pattern = patterns[0][0].rstrip("/")
        link_config = f'''  "links": {{
    "startUrls": [""],
    "pattern": "{best_pattern}",
    "maxDepth": 2
  }}'''
        link_note = f"  # Pattern covers {patterns[0][1]} links"
    else:
        fallback_pattern = (
            f"/{parsed.path.split('/')[1]}"
            if parsed.path and len(parsed.path.split("/")) > 1
            else ""
        )
        link_config = f'''  "links": {{
    "startUrls": [""],
    "pattern": "{fallback_pattern}",
    "maxDepth": 2
  }}'''
        link_note = "  # WARNING: No clear pattern found - verify this setting"

    # Print final config
    print(
        f'''
"your-site-id": {{
  "name": "Your Site Name",
  "baseUrl": "{data['base_url_suggestion']}",
  "mode": "fetch",
{link_config},
{link_note}
{content_config}
{method_note}
}}
'''
    )

    print("=" * 70)
    print("NEXT STEPS:")
    print("=" * 70)
    print(
        """
1. Review the suggested configuration above
2. Add it to scraper/config/sites.json (replace 'your-site-id')
3. Test link discovery:
   python docpull.py links your-site-id

4. Test content extraction (use a path from sample links):
   python docpull.py content your-site-id <path>

5. If tests pass, index the entire site:
   python docpull.py index your-site-id
"""
    )
    print("=" * 70)


@app.command()
def links(
    site_id: Annotated[str, typer.Argument(help="Site ID to get links for")],
    save: Annotated[bool, typer.Option("--save", help="Save links to ./data/<site_id>_links.json")] = False,
    force: Annotated[bool, typer.Option("--force", help="Force fresh crawl (bypass cache)")] = False,
):
    """Get all documentation links for a site."""
    params = {"max_age": 0} if force else {}
    resp = httpx.get(
        f"{API_BASE}/sites/{site_id}/links",
        params=params,
        headers=get_auth_headers(),
        timeout=120.0,
    )
    resp.raise_for_status()
    data = resp.json()
    for link in data["links"]:
        print(link)
    print(f"\nTotal: {data['count']} links", file=sys.stderr)

    if save:
        out_dir = "./data"
        os.makedirs(out_dir, exist_ok=True)
        out_path = f"{out_dir}/{site_id}_links.json"
        with open(out_path, "w") as f:
            json.dump({f"{site_id}_links": data["links"]}, f, indent=2)
        print(f"Saved to {out_path}", file=sys.stderr)


@app.command()
def content(
    site_id: Annotated[str, typer.Argument(help="Site ID")],
    path: Annotated[str, typer.Argument(help="Page path relative to baseUrl")],
    force: Annotated[bool, typer.Option("--force", help="Force fresh scrape (also clears error tracking)")] = False,
):
    """Get content from a specific page path."""
    params = {"path": path}
    if force:
        params["max_age"] = 0  # Force fresh scrape

    resp = httpx.get(
        f"{API_BASE}/sites/{site_id}/content",
        params=params,
        headers=get_auth_headers(),
        timeout=120.0,
    )
    resp.raise_for_status()
    data = resp.json()
    page_content = data["content"]

    # Sanitize path for filename
    safe_path = path.strip("/").replace("/", "_") or "index"
    out_dir = f"./docs/{site_id}"
    os.makedirs(out_dir, exist_ok=True)
    out_path = f"{out_dir}/{safe_path}.md"

    with open(out_path, "w") as f:
        f.write(page_content)

    cache_status = "(cached)" if data.get("from_cache") else "(fresh)"
    print(f"Saved to {out_path} ({len(page_content)} chars) {cache_status}")


@app.command()
def index(
    site_id: Annotated[str, typer.Argument(help="Site ID to index")],
    max_concurrent: Annotated[int, typer.Option("--max-concurrent", "-c", help="Max concurrent requests")] = 50,
):
    """Fetch and save all pages from a site using parallel bulk API."""
    print(f"Indexing {site_id}...", file=sys.stderr)

    # Use the parallel bulk indexing API endpoint
    resp = httpx.post(
        f"{API_BASE}/sites/{site_id}/index",
        params={"max_concurrent": max_concurrent},
        headers=get_auth_headers(),
        timeout=120.0,
    )
    resp.raise_for_status()
    data = resp.json()

    cached = data.get("cached", 0)
    scraped = data.get("scraped", 0)
    skipped = data.get("skipped_assets", 0)
    print(
        f"\nTotal: {data['total']} pages | Cached: {cached} | Scraped: {scraped}",
        file=sys.stderr,
    )
    if skipped:
        print(f"Skipped: {skipped} assets (PDFs, images, feeds)", file=sys.stderr)
    print(
        f"Success: {data['successful']} | Failed: {data['failed']}",
        file=sys.stderr,
    )
    if data.get("errors"):
        print("\nFirst 10 errors:", file=sys.stderr)
        for err in data["errors"]:
            print(f"  {err['path']}: {err['error']}", file=sys.stderr)


@app.command()
def download(
    site_id: Annotated[str, typer.Argument(help="Site ID to download")],
    output_dir: Annotated[str, typer.Option("--output", "-o", help="Output directory")] = ".",
):
    """Download all documentation for a site as a ZIP file."""
    print(f"Downloading {site_id} docs...", file=sys.stderr)

    resp = httpx.get(
        f"{API_BASE}/sites/{site_id}/download",
        headers=get_auth_headers(),
        timeout=120.0,
    )
    resp.raise_for_status()

    # Save ZIP file
    out_path = os.path.join(output_dir, f"{site_id}_docs.zip")
    with open(out_path, "wb") as f:
        f.write(resp.content)

    # Display stats from response headers
    total = resp.headers.get("X-Download-Total", "?")
    cached = resp.headers.get("X-Download-Cached", "?")
    scraped = resp.headers.get("X-Download-Scraped", "?")
    failed = resp.headers.get("X-Download-Failed", "?")

    print(f"Saved to {out_path} ({len(resp.content):,} bytes)", file=sys.stderr)
    print(
        f"Pages: {total} total | {cached} cached | {scraped} scraped | {failed} failed",
        file=sys.stderr,
    )


@app.command(name="export")
def export_cmd(
    urls_file: Annotated[str, typer.Argument(help="Path to file with URLs (one per line), or '-' for stdin")],
    output: Annotated[str, typer.Option("--output", "-o", help="Output ZIP file path")] = "docs_export.zip",
    unzip: Annotated[bool, typer.Option("--unzip", help="Auto-extract after download")] = False,
    scrape: Annotated[bool, typer.Option("--scrape", help="Scrape missing content (default: cached only)")] = False,
):
    """Export a list of URLs as a ZIP file.

    URLs are auto-resolved to configured sites using longest-prefix matching.
    By default only returns cached content (use --scrape to fetch fresh).
    """
    # Read URLs from file or stdin
    if urls_file == "-":
        urls = [line.strip() for line in sys.stdin if line.strip() and not line.startswith("#")]
    else:
        with open(urls_file) as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith("#")]

    if not urls:
        print("Error: No URLs provided", file=sys.stderr)
        raise typer.Exit(1)

    print(f"Exporting {len(urls)} URLs (cached_only={not scrape})...", file=sys.stderr)

    # Build request
    payload = {
        "urls": urls,
        "cached_only": not scrape,
        "include_manifest": True,
    }

    resp = httpx.post(
        f"{API_BASE}/export/zip",
        json=payload,
        headers=get_auth_headers(),
        timeout=600.0,  # 10 min for large exports
        follow_redirects=True,
    )
    resp.raise_for_status()

    # Save ZIP file
    with open(output, "wb") as f:
        f.write(resp.content)

    # Display stats from response headers
    total = resp.headers.get("X-Export-Total", "?")
    ok = resp.headers.get("X-Export-Ok", "?")
    cached = resp.headers.get("X-Export-Cached", "?")
    scraped = resp.headers.get("X-Export-Scraped", "?")
    miss = resp.headers.get("X-Export-Miss", "?")
    error = resp.headers.get("X-Export-Error", "?")

    print(f"Saved to {output} ({len(resp.content):,} bytes)", file=sys.stderr)
    print(
        f"URLs: {total} total | {ok} ok (cached: {cached}, scraped: {scraped}) | {miss} miss | {error} error",
        file=sys.stderr,
    )

    # Auto-unzip if requested
    if unzip:
        import zipfile

        print("Extracting to current directory...", file=sys.stderr)
        with zipfile.ZipFile(output, "r") as zf:
            zf.extractall(".")
        print("Extracted to ./docs/", file=sys.stderr)


# --- Bulk Job Commands ---


@app.command()
def bulk(
    urls_file: Annotated[str, typer.Argument(help="Path to file with URLs (one per line), or '-' for stdin")],
):
    """Submit a bulk scrape job (fire-and-forget).

    Unlike 'export', this spawns parallel workers and returns immediately.
    Use 'job <job_id> --watch' to monitor progress.

    Accepts plain text (one URL per line) or JSON files (with *_links array).
    """
    # Read URLs from file or stdin
    if urls_file == "-":
        urls = [line.strip() for line in sys.stdin if line.strip() and not line.startswith("#")]
    else:
        with open(urls_file) as f:
            content = f.read()

        # Try JSON first (for *_links.json files)
        if urls_file.endswith(".json"):
            try:
                data = json.loads(content)
                # Find the first key ending in _links
                for key in data:
                    if key.endswith("_links") and isinstance(data[key], list):
                        urls = data[key]
                        break
                else:
                    # Fallback: if it's a list directly
                    urls = data if isinstance(data, list) else []
            except json.JSONDecodeError:
                urls = []
        else:
            # Plain text: one URL per line
            urls = [line.strip() for line in content.splitlines() if line.strip() and not line.startswith("#")]

    if not urls:
        print("Error: No URLs provided", file=sys.stderr)
        raise typer.Exit(1)

    print(f"Submitting bulk job with {len(urls)} URLs...", file=sys.stderr)

    resp = httpx.post(
        f"{API_BASE}/jobs/bulk",
        json={"urls": urls},
        headers=get_auth_headers(),
        timeout=60.0,
    )
    resp.raise_for_status()
    data = resp.json()

    job_id = data.get("job_id")
    if not job_id:
        print("No scrapeable URLs found", file=sys.stderr)
        raise typer.Exit(0)

    print(f"\nJob submitted: {job_id}", file=sys.stderr)
    print(f"Batches: {data.get('batches', 0)}", file=sys.stderr)
    print(f"Sites: {data.get('input', {}).get('sites', [])}", file=sys.stderr)
    print(f"To scrape: {data.get('input', {}).get('to_scrape', 0)}", file=sys.stderr)
    print(f"\nWatch progress: python docpull.py job {job_id} --watch", file=sys.stderr)
    print(job_id)  # Print job_id to stdout for scripting


@app.command()
def job(
    job_id: Annotated[str, typer.Argument(help="Job ID to check")],
    watch: Annotated[bool, typer.Option("--watch", "-w", help="Watch until completion")] = False,
    interval: Annotated[int, typer.Option("--interval", "-i", help="Watch interval in seconds")] = 2,
):
    """Check status of a bulk scrape job."""

    def fetch_status():
        resp = httpx.get(
            f"{API_BASE}/jobs/{job_id}",
            headers=get_auth_headers(),
            timeout=30.0,
        )
        resp.raise_for_status()
        return resp.json()

    def print_status(data):
        print(f"Job: {data.get('job_id')}")
        print(f"Status: {data.get('status')}")
        print(f"Progress: {data.get('progress_pct', 0)}%")
        print(f"Elapsed: {data.get('elapsed_seconds', 0):.1f}s")
        print(f"\nInput:")
        inp = data.get("input", {})
        print(f"  Total URLs: {inp.get('total_urls', 0)}")
        print(f"  To scrape: {inp.get('to_scrape', 0)}")
        print(f"  Assets: {inp.get('assets', 0)}")
        print(f"  Unknown: {inp.get('unknown', 0)}")
        print(f"  Sites: {inp.get('sites', [])}")
        print(f"\nProgress:")
        prog = data.get("progress", {})
        print(f"  Completed: {prog.get('completed', 0)}")
        print(f"  Success: {prog.get('success', 0)}")
        print(f"  Skipped: {prog.get('skipped', 0)}")
        print(f"  Failed: {prog.get('failed', 0)}")
        workers = data.get("workers", {})
        print(f"\nWorkers: {workers.get('completed', 0)}/{workers.get('total', 0)}")
        errors = data.get("errors", [])
        if errors:
            print(f"\nErrors ({len(errors)}):")
            for err in errors[:10]:
                print(f"  {err.get('path', '?')}: {err.get('error', '?')[:60]}")

    if not watch:
        data = fetch_status()
        print_status(data)
        return

    # Watch mode
    while True:
        data = fetch_status()
        # Clear screen and print
        print("\033[2J\033[H", end="")  # Clear screen
        print_status(data)

        if data.get("status") == "completed":
            break
        time.sleep(interval)


@app.command()
def jobs(
    limit: Annotated[int, typer.Option("--limit", "-n", help="Max jobs to show")] = 20,
):
    """List recent bulk scrape jobs."""
    resp = httpx.get(
        f"{API_BASE}/jobs",
        params={"limit": limit},
        headers=get_auth_headers(),
        timeout=30.0,
    )
    resp.raise_for_status()
    data = resp.json()

    jobs_list = data.get("jobs", [])
    if not jobs_list:
        print("No jobs found", file=sys.stderr)
        return

    # Print header
    print(f"{'JOB ID':<10} {'STATUS':<12} {'PROGRESS':<12} {'SITES'}")
    print("-" * 60)
    for j in jobs_list:
        sites = ", ".join(j.get("sites", [])[:3])
        if len(j.get("sites", [])) > 3:
            sites += f" +{len(j['sites']) - 3}"
        print(f"{j.get('job_id', '?'):<10} {j.get('status', '?'):<12} {j.get('progress', '?'):<12} {sites}")


# --- Cache subcommands ---


@cache_app.command(name="stats")
def cache_stats():
    """Show cache statistics."""
    resp = httpx.get(f"{API_BASE}/cache/stats", timeout=30.0)
    resp.raise_for_status()
    data = resp.json()
    print(f"Total cache entries: {data['total_entries']}")
    print("\nBy type:")
    for type_name, count in data["by_type"].items():
        print(f"  {type_name}: {count}")
    print("\nBy site:")
    for site, count in data["by_site"].items():
        print(f"  {site}: {count}")


@cache_app.command(name="keys")
def cache_keys(
    site_id: Annotated[Optional[str], typer.Argument(help="Filter by site ID")] = None,
):
    """List all cached URLs (pipe to export)."""
    params = {"content_only": "true"}
    if site_id:
        params["site_id"] = site_id
    resp = httpx.get(
        f"{API_BASE}/cache/keys", params=params, timeout=60.0
    )
    resp.raise_for_status()
    data = resp.json()
    for entry in data["keys"]:
        print(entry["url"])
    print(f"\nTotal: {data['count']} cached pages", file=sys.stderr)


@cache_app.command(name="clear")
def cache_clear(
    site_id: Annotated[str, typer.Argument(help="Site ID to clear cache for")],
):
    """Clear cache for a site."""
    resp = httpx.delete(f"{API_BASE}/cache/{site_id}", timeout=30.0)
    resp.raise_for_status()
    print(f"Cleared {resp.json()['deleted']} cache entries for {site_id}")


if __name__ == "__main__":
    app()
